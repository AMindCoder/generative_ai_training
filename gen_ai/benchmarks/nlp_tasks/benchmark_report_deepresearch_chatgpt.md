# Industry-Standard Benchmarks for LLM Evaluation Across Modalities

## Text Benchmarks (NLP)  

### GLUE: General Language Understanding Evaluation  
The **GLUE** benchmark is a collection of nine English NLU tasks, from single-sentence classification (linguistic acceptability) to sentence-pair tasks (paraphrase detection, natural language inference, etc.) ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=GLUE%3A%20Single,Paraphrase%20Tasks%20and%20Inference%20Tasks)) ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=Roughly%20one%20year%20after%20the,Textual%20Entailment%20and%20the%20Winograd)). Models are evaluated on each task using the task-specific metric (e.g. accuracy or F1), and an overall score is the macro-average across tasks. GLUE was introduced in 2018 to provide a standard yardstick for text models ([Killed by LLM](https://r0bk.github.io/killedbyllm/#:~:text=Killed%205%20years%20ago%2C%20A,year%20and%201%20months%20old)). The dataset draws from established corpora (e.g. SST-2 for sentiment, QQP for paraphrase, MNLI for inference) and includes a public leaderboard ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=GLUE%20General%20Language%20Understanding%20%E2%80%94,%E2%80%94%20Eight%20different%20NLU%20tasks)). For example, the BERT model achieved around 80–82 average score, surpassing prior baselines around 70 ([[1905.10425] Human vs. Muppet: A Conservative Estimate of Human ...](https://arxiv.org/abs/1905.10425#:~:text=The%20GLUE%20benchmark%20,We%20provide%20a)). Humans perform at ~87.1 on GLUE ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=The%20human%20baseline%20score%20is,6)), and by 2019 models like **XLNet** slightly exceeded this (e.g. ~88.4) ([Killed by LLM](https://r0bk.github.io/killedbyllm/#:~:text=Original%20Score)). Today, many models (RoBERTa, T5, etc.) top **90** on GLUE, indicating near-saturation ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=The%20human%20baseline%20score%20is,6)). GLUE’s straightforward tasks helped kickstart rapid progress in language model pre-training, but also showed that models could exploit spurious patterns, prompting development of a tougher benchmark ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=By%20late%202019%2C%20models%20like,need%20for%20a%20harder%20benchmark)) ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=Roughly%20one%20year%20after%20the,Textual%20Entailment%20and%20the%20Winograd)).  

### SuperGLUE: Harder Language Understanding Tasks  
**SuperGLUE** (2019) is a successor to GLUE with more challenging tasks that require reasoning and context ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=One%20of%20the%20biggest%20differences,is%20the%20complexity%20of%20tasks)) ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=,examples%20to%20ensure%20robust%20evaluation)). It comprises 8 tasks, including the **Winograd Schema Challenge** (coreference resolution that demands commonsense), logical inference (Boolean QA), and reading comprehension with unanswerable questions ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=classification%20and%20similarity%20detection.%20,logical%20inference%20and%20contextual%20awareness)) ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=For%20example%2C%20SuperGLUE%20includes%20Winograd,to%20solve%20without%20genuine%20comprehension)). Unlike GLUE’s simple averaging, SuperGLUE introduced a difficulty-weighted average score and reported human performance (~89.8) for reference ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=method%20does%20not%20accurately%20represent,the%20difficulty%20of%20each%20task)) ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=For%20SuperGLUE%2C%20the%20human%20baseline,3)). Initial state-of-the-art models like ALBERT and T5 approached but did not exceed human-level on SuperGLUE (e.g. T5 Large scored 89.3 versus the 89.8 human benchmark) ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=For%20SuperGLUE%2C%20the%20human%20baseline,3)). This benchmark “stayed alive” longer – even as of 2021, most models were still below human parity ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=GLUE%2C%20highlighting%20the%20need%20for,a%20harder%20benchmark)). The harder tasks forced models to improve at nuanced reasoning rather than just pattern matching ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=One%20of%20the%20biggest%20differences,is%20the%20complexity%20of%20tasks)) ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=Early%20GLUE%20models%20often%20found,rather%20than%20truly%20understanding%20language)). By now, the best LLMs have dramatically improved: for instance, GPT-4 reportedly scores **~97.3** on the SuperGLUE scale, outperforming humans and prior models by a wide margin ([6 Cutting-Edge Pretrained Models for Text Classification in 2024](https://www.33rdsquare.com/6-pretrained-models-text-classification/#:~:text=On%20the%20SuperGLUE%20benchmark%2C%20a,a%20handful%20of%20labeled)). This indicates that cutting-edge LLMs have largely conquered these once-challenging NLP tasks.

### MMLU: Massive Multitask Language Understanding  
The **MMLU** benchmark tests broad knowledge and reasoning across 57 diverse subjects ([MMLU benchmark overview | arduin.io](https://arduin.io/blog/mmlu-overview/#:~:text=available%20benchmarks%20such%20as%20SuperGLUE,as%20listed%20in%20the%20paper)). It includes ~14,000 multiple-choice questions spanning elementary math, history, law, college biology, astronomy, etc., designed to probe a language model’s retention of **world knowledge** and problem-solving in both STEM and humanities ([MMLU benchmark overview | arduin.io](https://arduin.io/blog/mmlu-overview/#:~:text=available%20benchmarks%20such%20as%20SuperGLUE,as%20listed%20in%20the%20paper)). Models are evaluated by accuracy on 4-way multiple choice questions. A random guess yields 25%, and crowd workers (non-expert humans) score around 34.5% ([MMLU benchmark overview | arduin.io](https://arduin.io/blog/mmlu-overview/#:~:text=would%20in%20expectation%20still%20score,8)), whereas an expert human is estimated to achieve ~89.8% ([MMLU benchmark overview | arduin.io](https://arduin.io/blog/mmlu-overview/#:~:text=would%20in%20expectation%20still%20score,8)). MMLU has become a key **benchmark for LLMs’ depth of understanding** – for example, GPT-4 scored **86.4%** on MMLU, far surpassing its predecessor GPT-3.5 (around 70%) and even exceeding other large models like Google’s PaLM (71%) ([What is GPT-4? | ITPro](https://www.itpro.com/technology/artificial-intelligence-ai/368288/what-is-gpt-4#:~:text=OpenAI%20also%20claims%20that%20GPT,3.5%E2%80%99s%2070%25%20and%20PaLM%E2%80%99s%2071)). This is just shy of the human-expert level, and notably OpenAI, Anthropic, and Google all highlight MMLU in model releases ([MMLU benchmark overview | arduin.io](https://arduin.io/blog/mmlu-overview/#:~:text=The%20Massive%20Multitask%20Language%20Understanding,Despite)). In fact, Google DeepMind’s **Gemini** reportedly reached **90%** on MMLU’s English questions, slightly above the human expert benchmark ([Google's Gemini: Setting new benchmarks in language models](https://www.superannotate.com/blog/google-gemini-ai#:~:text=Google%27s%20Gemini%3A%20Setting%20new%20benchmarks,images%2C%20including%20reading%20comprehension)). Such results demonstrate how top models are now at or above human-level on a broad array of exam-style questions. MMLU’s diverse coverage makes it a robust test: even if a model excels in one area, the benchmark will expose weaknesses in others ([MMLU: Better Benchmarking for LLM Language Understanding](https://deepgram.com/learn/mmlu-llm-benchmark-guide#:~:text=MMLU%3A%20Better%20Benchmarking%20for%20LLM,researchers%20to%20address%20weaknesses)). As models improve, MMLU scores inch closer to 90+%, but the benchmark remains useful for pinpointing specific subject domains where an AI might lag behind.  

### Other Notable NLP Benchmarks  
Beyond the above, there are many benchmarks targeting specific aspects of language understanding. **SQuAD** (Stanford QA Dataset) focuses on reading comprehension – top models now exceed human performance on extracting exact answers from passages (human F1 ~89.5, models ~92.8) ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=Humans%20achieve%20an%20EM%20score,452)) ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=Currently%2C%20the%20best%20performing%20model,777)). **BIG-bench** is a collection of over 200 challenging tasks (from logic puzzles to coding problems) to probe complex or niche capabilities ([Killed by LLM](https://r0bk.github.io/killedbyllm/#:~:text=in%20many%20papers%20i.e.%20GPT,technical%20report)). For instance, BIG-bench highlighted areas like commonsense reasoning and math where early GPT-series models struggled; a 540B model (PaLM) achieved ~61% on BIG-bench, outperforming a 50% human baseline ([Killed by LLM](https://r0bk.github.io/killedbyllm/#:~:text=Killed%202%20years%20ago%2C%20A,It%20was%2010%20months%20old)) ([Killed by LLM](https://r0bk.github.io/killedbyllm/#:~:text=Human%3A%2049.8)). Other benchmarks target **common sense and truthfulness** (e.g. HellaSwag for commonsense inference, TruthfulQA for detecting false or misleading model outputs). In sum, the NLP field has a rich suite of tests: GLUE/SuperGLUE for general language understanding, MMLU and BIG-bench for broad knowledge and reasoning, and numerous specialized evaluations for translation, summarization, logic, and beyond. Each benchmark provides a different lens on an LLM’s abilities, and together they drive models toward more **robust and general** language understanding.  ([Benchmarks in AI: Measuring and Comparing Model Performance](https://www.forwardfuture.ai/p/benchmarks-in-artificial-intelligence-measuring-comparing-understanding#:~:text=At%20the%20beginning%20of%20AI,sophisticated%20tests%20have%20become%20necessary)) ([Benchmarks in AI: Measuring and Comparing Model Performance](https://www.forwardfuture.ai/p/benchmarks-in-artificial-intelligence-measuring-comparing-understanding#:~:text=The%20rapid%20development%20of%20large,and%20what%20criticism%20they%20attract))

 ([MMLU benchmark overview | arduin.io](https://arduin.io/blog/mmlu-overview/#:~:text=Image)) ([MMLU benchmark overview | arduin.io](https://arduin.io/blog/mmlu-overview/)) *Major AI labs now prominently report benchmark scores like MMLU when launching new models. For example, OpenAI’s GPT-4 (86.4% on MMLU), Anthropic’s Claude 3 (up to 86.8%), and Google’s Gemini (90%) were compared against an ~89.8% expert-human level on this broad test of knowledge ([Google's Gemini: Setting new benchmarks in language models](https://www.superannotate.com/blog/google-gemini-ai#:~:text=Pichai%20notes%20that%20on%20the,images%2C%20including%20reading%20comprehension)) ([What is GPT-4? | ITPro](https://www.itpro.com/technology/artificial-intelligence-ai/368288/what-is-gpt-4#:~:text=OpenAI%20also%20claims%20that%20GPT,3.5%E2%80%99s%2070%25%20and%20PaLM%E2%80%99s%2071)).*  

## Image Benchmarks (Vision and Vision-Language)  

### ImageNet: Object Recognition at Scale  
**ImageNet** is a landmark image classification benchmark that propelled modern computer vision. It consists of 1.2 million training images across 1,000 object categories, from animals to everyday objects ([The Kinetics Human Action Video Dataset - arXiv.org](https://arxiv.org/pdf/1705.06950#:~:text=Statistics%3A%20The%20dataset%20has%20400,one%20for%20testing%20with)). The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates models on a 50,000-image validation set and a test set, using **top-1 and top-5 accuracy** (i.e. whether the correct label is the model’s first guess or within its top-5 guesses). ImageNet’s methodology is straightforward classification – each image has one correct label. Early CNN models achieved around 50-70% top-1 accuracy; the 2012 **AlexNet** model famously reached ~84.7% top-5 accuracy (16% error) and sparked the deep learning revolution. Human performance on this task is estimated around 95% top-5 accuracy ([Top 7 Baselines For State-of-the-art Image Recognition Models](https://analyticsindiamag.com/ai-trends/image-classification-models-benchmark-state-of-the-art/#:~:text=Training%20a%20model%20on%20ImageNet,team%20in%20collaboration%20with)). Over the years, architectures like VGG, ResNets, and Vision Transformers pushed accuracy higher. Today, state-of-the-art vision models exceed **88% top-1** accuracy ([Top 7 Baselines For State-of-the-art Image Recognition Models](https://analyticsindiamag.com/ai-trends/image-classification-models-benchmark-state-of-the-art/#:~:text=Training%20a%20model%20on%20ImageNet,team%20in%20collaboration%20with)) and ~98% top-5. For example, Google’s NoisyStudent (efficiently leveraging unlabeled data) hit ~88.4% top-1 ([Top 7 Baselines For State-of-the-art Image Recognition Models](https://analyticsindiamag.com/ai-trends/image-classification-models-benchmark-state-of-the-art/#:~:text=Training%20a%20model%20on%20ImageNet,team%20in%20collaboration%20with)), and newer models like CoAtNet and ViT-g have inched toward 90+%. ImageNet remains a **baseline benchmark** – it’s well-studied and many models now perform at near-human level, but it mainly evaluates single-object identification in relatively clean images. Thus, it’s often complemented by harder tests (ImageNet-V2 for distribution shift, adversarial sets, or broader tasks) to further challenge models.

### COCO: Object Detection, Segmentation, and Captioning  
The **MS COCO** dataset (Common Objects in Context) is a comprehensive benchmark for **object detection** and image understanding in cluttered scenes. COCO contains on the order of **330,000 images** (200K labeled) covering 80 object categories in everyday settings ([COCO Dataset Resolution Insights and Applications](https://www.labellerr.com/blog/exploring-the-coco-dataset/#:~:text=The%20COCO%20dataset%20consists%20of,various%20objects%20in%20different%20contexts)) ([COCO Dataset Resolution Insights and Applications](https://www.labellerr.com/blog/exploring-the-coco-dataset/#:~:text=everyday%20scenes%20with%20various%20objects,in%20different%20contexts)). Each image has multiple objects with detailed annotations: bounding boxes, segmentation masks outlining object shapes, and even image captions describing the scene ([COCO Dataset Resolution Insights and Applications](https://www.labellerr.com/blog/exploring-the-coco-dataset/#:~:text=Figure%3A%20Samples%20of%20annotated%20images,in%20the%20MS%20COCO%20dataset)). Unlike ImageNet, which has one label per image, COCO evaluates a model’s ability to detect *all* objects and label them correctly in each image. The primary metric for COCO detection is **mean Average Precision (mAP)**, averaged over different intersection-over-union (IoU) thresholds (commonly AP@0.50:0.95) ([Breaking Beyond COCO Object Detection | OpenReview](https://openreview.net/forum?id=hj7uBF92qvm#:~:text=Breaking%20Beyond%20COCO%20Object%20Detection,from)). This metric rewards accurate localization and identification of every object in the scene. COCO’s complexity is significantly higher: images may contain many objects (people, animals, vehicles, etc.) in varied orientations and occlusions, requiring **contextual understanding**. For instance, one image might have a person on a sofa with a cat on their lap – a model must detect *person*, *sofa*, and *cat* with tight bounding boxes. Top-performing models (often based on Region CNNs, YOLO, or Transformer detectors like DETR) have achieved around **50–60% mAP** on COCO’s test set ([State of the Art: Object detection (2/2) – AI Code Wizards](https://aicodewizards.com/2022/05/22/sota_object_detection_2/#:~:text=Wizards%20aicodewizards.com%20%20EfficientDet,AP%20on%20COCO%20test)) ([Breaking Beyond COCO Object Detection | OpenReview](https://openreview.net/forum?id=hj7uBF92qvm#:~:text=bound%2C%20and%20errors%20in%20models,from)). For example, an EfficientDet variant reached ~55.1 mAP ([State of the Art: Object detection (2/2) – AI Code Wizards](https://aicodewizards.com/2022/05/22/sota_object_detection_2/#:~:text=Wizards%20aicodewizards.com%20%20EfficientDet,AP%20on%20COCO%20test)), and more recent transformer-based models push close to 60. (By comparison, human annotators have near 85–90% on this metric in controlled settings ([Breaking Beyond COCO Object Detection | OpenReview](https://openreview.net/forum?id=hj7uBF92qvm#:~:text=Breaking%20Beyond%20COCO%20Object%20Detection,from)), indicating COCO still has headroom). COCO is also used for **image captioning** (evaluated by BLEU, CIDEr scores) and **keypoint detection** (e.g. human pose estimation), making it a versatile benchmark for vision and vision-language tasks. Its **“objects in context”** philosophy means models must handle real-world complexity – a major step up from ImageNet’s isolated objects.

 ([COCO Dataset Resolution Insights and Applications](https://www.labellerr.com/blog/exploring-the-coco-dataset/#:~:text=Figure%3A%20Samples%20of%20annotated%20images,in%20the%20MS%20COCO%20dataset)) ([COCO Dataset Resolution Insights and Applications](https://www.labellerr.com/blog/exploring-the-coco-dataset/)) *Examples of MS COCO images with annotations. Each image contains multiple objects (people, animals, vehicles, etc.) marked with bounding boxes and segmentation masks (colored regions), reflecting the challenge of detecting and segmenting objects in complex scenes ([COCO Dataset Resolution Insights and Applications](https://www.labellerr.com/blog/exploring-the-coco-dataset/#:~:text=The%20COCO%20dataset%20consists%20of,various%20objects%20in%20different%20contexts)) ([COCO Dataset Resolution Insights and Applications](https://www.labellerr.com/blog/exploring-the-coco-dataset/#:~:text=Each%20image%20is%20carefully%20annotated,objects%20present%20in%20the%20scene)).*  

### VQAv2: Visual Question Answering  
The **VQAv2** benchmark evaluates **multimodal understanding** – given an image and a natural language question about that image, a model must provide a correct free-form answer. The dataset contains ~1.1 million questions posed on ~250,000 images (drawn from COCO and others) ([VQA: Visual Question Answering](https://visualqa.org/challenge.html#:~:text=VQA%3A%20Visual%20Question%20Answering%20The,edition%20of%20the%20VQA%20Challenge)). Questions range from simple (“What is the color of the cat?”) to complex (“Why are the people gathered here?”) requiring visual reasoning and commonsense. Each question has 10 ground-truth answers from human annotators, since there can be variability (e.g. one person might answer “kitty” vs. “cat”). The evaluation uses an **accuracy metric** that credits an answer if it matches the majority of annotators – specifically, an answer earns full points if at least 3 of 10 humans gave that exact answer, accounting for slight wording differences ([VQA: Visual Question Answering](https://visualqa.org/evaluation.html#:~:text=VQA%3A%20Visual%20Question%20Answering%20We,9%20sets%20of%20human%20annotators)). This metric is capped at 100% when the model’s answer is at least as good as human consensus. VQAv2 was introduced to reduce language biases present in VQAv1 (by balancing question types and images). Performance on VQAv2 has steadily improved: early models (circa 2016) were around 50–60% accuracy ([Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA ...](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Adversarial_VQA_A_New_Benchmark_for_Evaluating_the_Robustness_of_ICCV_2021_paper.pdf#:~:text=,VQA%20v2%20and%20other%20robust)). With the advent of better visual features (from CNN backbones) and attention-based multimodal fusion (e.g. the 2018 Bottom-Up & Top-Down model), accuracy rose into the 70s. The current state-of-the-art (e.g. **BEiT-3** vision-language transformer) achieves roughly **75–77%** on the VQAv2 test-standard benchmark ([Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA ...](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Adversarial_VQA_A_New_Benchmark_for_Evaluating_the_Robustness_of_ICCV_2021_paper.pdf#:~:text=,VQA%20v2%20and%20other%20robust)). For context, human performance is about 80–85% on this task (because even humans occasionally give different answers or misunderstand the question) ([Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA ...](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Adversarial_VQA_A_New_Benchmark_for_Evaluating_the_Robustness_of_ICCV_2021_paper.pdf#:~:text=,VQA%20v2%20and%20other%20robust)) ([VQA: Visual Question Answering](https://visualqa.org/evaluation.html#:~:text=VQA%3A%20Visual%20Question%20Answering%20We,9%20sets%20of%20human%20annotators)). VQA is considered a challenging, **real-world applicable test**: a good score implies the model can parse image content and language, then reason about them together. It’s especially relevant for multimodal AI systems like GPT-4 Vision, which are expected to answer questions about images.  

### CLIP-Based Evaluations: Zero-Shot Classification and Image-Text Alignment  
OpenAI’s **CLIP** (Contrastive Language–Image Pretraining) model introduced a powerful way to evaluate image and text understanding jointly. CLIP is trained on hundreds of millions of image-caption pairs and learns to align visual and textual representations. Thanks to this, CLIP and similar models enable **zero-shot image classification**: evaluating on datasets like ImageNet *without fine-tuning*. For example, CLIP ViT-L/14 (a 428M-parameter model) achieves around **76%** top-1 accuracy on ImageNet in a zero-shot setting ([CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy ...](https://openreview.net/pdf?id=0hTtit3AAm#:~:text=CLIPA,By%20upscaling)). Improved training (OpenCLIP, etc.) has pushed this to **80%+** zero-shot ImageNet accuracy ([CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy ...](https://arxiv.org/abs/2306.15658#:~:text=CLIPA,39X)) ([CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy ...](https://openreview.net/pdf?id=0hTtit3AAm#:~:text=CLIPA,By%20upscaling)) – remarkably high given the model wasn’t explicitly trained on ImageNet labels. Such benchmarks highlight a model’s ability to **generalize** via language: one simply provides text prompts like “a photo of a _” for each class and sees if the model picks the correct label for the image. CLIP-based evaluations also include **image-text retrieval** (retrieving the correct caption for an image or vice versa, measured by recall@K) and **CLIPScore** for caption quality. **CLIPScore** is a reference-free metric that computes the cosine similarity between a generated caption and the image embedding; it has been shown to correlate well with human judgment on captioning tasks ([CLIP Score — PyTorch-Metrics 1.6.2 documentation - Lightning](https://lightning.ai/docs/torchmetrics/stable/multimodal/clip_score.html#:~:text=CLIP%20Score%20%E2%80%94%20PyTorch,of%20the%20image%2C%20as)). In practice, CLIP-based benchmarks are used to test multimodal models’ alignment: for instance, a new multimodal LLM might be evaluated on zero-shot classification (via CLIP’s method) or on retrieval tasks to ensure it properly associates visual concepts with language. These evaluations are **crucial for models like GPT-4V or Google’s multi-modal PaLM-E**, as they gauge if the model has acquired the same rich image-text representations that CLIP has. In summary, CLIP-style benchmarks extend vision evaluation beyond pure detection or captioning – they test a model’s **flexibility and semantic alignment** between modalities, often under zero-shot or few-shot conditions that mimic real-world deployment.  

## Audio Benchmarks (Speech Recognition and Understanding)  

### LibriSpeech: Automatic Speech Recognition (ASR)  
**LibriSpeech** is a widely used benchmark dataset for speech-to-text, derived from public domain audiobook recordings (LibriVox). It offers around **1000 hours** of English read speech, segmented into training sets (e.g. 100h “clean”, 360h “clean”, 500h “other”) and test sets ([librispeech_asr · Datasets at Model Database](https://modeldatabase.com/datasets/librispeech_asr.html#:~:text=Dataset%20Card%20for%20librispeech_asr%20Dataset,Supported%20Tasks%20and%20Leaderboards)). The “clean” portions are high-quality audio with clear speech, while “other” portions contain more challenging acoustic conditions or accents. LibriSpeech’s evaluation focuses on **Automatic Speech Recognition**: converting spoken audio to text, measured by **Word Error Rate (WER)**. WER counts the percentage of words that are substituted, deleted, or inserted compared to the reference transcript (lower WER is better). For example, a WER of 5% means 5 out of 100 words on average are wrong. LibriSpeech enabled ASR research to standardize progress – older models like DeepSpeech (2016) had WERs ~14% on test-clean. Thanks to techniques like seq-to-seq with attention and then self-supervised pretraining (wav2vec 2.0, etc.), errors plummeted. Human transcribers on LibriSpeech test-clean are estimated around ~5% WER (and ~12% on test-other due to harder speech) ([Benchmarks November 2024 - withaqua.com](https://withaqua.com/blog/benchmark-nov-2024#:~:text=Benchmarks%20November%202024%20,stringent%20latency%20requirements%2C%20often)). Today’s top models approach *super-human* performance on this benchmark: Facebook’s wav2vec 2.0 + self-training achieved **1.4% WER on test-clean and ~2.6% on test-other** ([new sota on Librispeech with semi-supervised learning #48 - GitHub](https://github.com/syhw/wer_are_we/issues/48#:~:text=new%20sota%20on%20Librispeech%20with,art)). Another example, an ensemble “United ASR” model reached ~1.9% on clean ([new sota on Librispeech with semi-supervised learning #48 - GitHub](https://github.com/syhw/wer_are_we/issues/48#:~:text=new%20sota%20on%20Librispeech%20with,art)). These are incredibly low error rates, essentially indicating near-perfect transcription on read English. The LibriSpeech benchmark (with its clear enunciation and reading-style sentences) is now considered **solved or nearly solved** by cutting-edge ASR models. However, it remains a **valuable baseline** and is still used in the SUPERB benchmark (for phoneme recognition and ASR tasks) to evaluate new models’ speech encoding quality ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=Phoneme%20Recognition%20,PER)) ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=Automatic%20Speech%20Recognition%20,WER)). When moving beyond LibriSpeech, researchers often look at more challenging conversational or noisy benchmarks, but LibriSpeech’s consistent setup makes it a gold standard for ASR progress.

### Common Voice: Diverse, Multilingual Speech Recognition  
Mozilla’s **Common Voice** project provides a massively multilingual speech dataset aimed at making ASR accessible for many languages. Unlike curated audiobooks, Common Voice consists of **crowd-sourced voice recordings** of volunteers reading text sentences. It currently spans **120+ languages** with over **20,000 hours** of validated speech in the latest versions ([Common Voice - Mozilla](https://commonvoice.mozilla.org/cv/datasets#:~:text=Common%20Voice%20,the%20Common%20Voice%20mailing%20list)) ([Common Voice - allizom.org](https://sandbox.commonvoice.allizom.org/en/datasets#:~:text=Common%20Voice%20,adding%20more%20voices%20and%20languages)). This benchmark is less of a single task and more of a platform: one can evaluate a model’s WER on Common Voice for a particular language or in aggregate. The key is that Common Voice captures a **wide variety of speakers**, accents, and recording conditions (microphone quality, background noise) – reflecting real-world data. For example, the English Common Voice set includes speakers of different age, gender, and accent, making it more challenging than LibriSpeech’s relatively homogeneous audio. The **methodology** for evaluation is typically the same (WER), but one must report it per language. Common Voice has a **leaderboard** for each language’s ASR (often hosted on Hugging Face or similar) ([Dataset Card for Common Voice Corpus 15 - Hugging Face](https://huggingface.co/datasets/mozilla-foundation/common_voice_15_0#:~:text=Dataset%20Card%20for%20Common%20Voice,Languages)). As an illustration of difficulty: a model that gets 2% WER on LibriSpeech might get higher error on Common Voice English due to more casual speaking styles. For many languages in Common Voice, commercial ASR systems still have high WER (20%+), especially for smaller datasets. Thus, Common Voice is a benchmark of **robustness and multilingual capability** – it tests whether an ASR model can generalize to different languages and accents. It has driven research into multilingual ASR models and accent adaptation. In practice, top systems often fine-tune large pre-trained models (like wav2vec 2.0 or Whisper) on the Common Voice data and report WER improvements. The Common Voice benchmark’s real-world diversity makes it highly relevant for evaluating LLM-based speech systems as well, especially those that promise multi-language understanding or that integrate speech input. Success on Common Voice indicates a model is **robust to varied speakers and languages**, moving closer to human-level listening across the globe.  

### SUPERB: Comprehensive Speech Processing Evaluation  
The **Speech processing Universal PERformance Benchmark (SUPERB)** is a benchmark suite designed to evaluate a single model’s capability across a **wide range of speech tasks** ([[2105.01051] SUPERB: Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051#:~:text=However%2C%20the%20speech%20processing%20community,demonstrate%20that%20the%20framework%20is)). Instead of focusing only on transcription, SUPERB covers multiple aspects of speech understanding: from low-level tasks like **phoneme recognition** to high-level understanding like **intent classification**. The benchmark uses a frozen pre-trained speech model (e.g. a wav2vec2 encoder) and adds lightweight task-specific heads, assessing how well the model’s learned representations can serve each task ([[2105.01051] SUPERB: Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051#:~:text=However%2C%20the%20speech%20processing%20community,demonstrate%20that%20the%20framework%20is)). SUPERB’s tasks span four categories: **Content**, **Semantics**, **Speaker**, and **Paralinguistics** ([SUPERB: Speech processing Universal PERformance Benchmark](https://sls.csail.mit.edu/publications/2021/JeffLai_Interspeech_2021.pdf#:~:text=Benchmark%20sls,PR%20transcribes%20an%20utterance%20into)). For content: Phoneme Recognition on LibriSpeech (metric: Phone Error Rate) tests if phonetic details are captured ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=Phoneme%20Recognition%20,PER)), and full ASR on LibriSpeech (WER) checks word-level transcription ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=Automatic%20Speech%20Recognition%20,WER)). For semantics: Intent Classification (on Fluent Speech Commands, metric: accuracy) evaluates understanding of spoken commands ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=,ic)), and Slot Filling tests semantic parsing of utterances ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=,sf)). For speaker-related tasks: Speaker Identification (using VoxCeleb1, metric: accuracy) asks the model to recognize who is speaking from voice characteristics ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=,si)), and Speaker Verification does one-to-one voice matching. Paralinguistic tasks include **Keyword Spotting** (detecting if a keyword was spoken, e.g. “Alexa,” measured by accuracy) ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=,ks)) and **Emotion Recognition** (classifying the speaker’s emotion from tone). By evaluating all these tasks, SUPERB provides a **holistic scorecard** of a model’s speech prowess. For example, a model like wav2vec 2.0 or HuBERT can be compared to another on how well it performs across the board – maybe one is slightly better at ASR, but worse at speaker identification, etc. Initial results when SUPERB was introduced showed that models pre-trained on large audio corpora significantly outperform older approaches on most tasks, demonstrating the value of self-supervised learning for speech ([[2105.01051] SUPERB: Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051#:~:text=with%20minimal%20architecture%20changes%20and,learning%20and%20general%20speech%20processing)) ([[2105.01051] SUPERB: Speech processing Universal PERformance Benchmark](https://arxiv.org/abs/2105.01051#:~:text=However%2C%20the%20speech%20processing%20community,demonstrate%20that%20the%20framework%20is)). SUPERB also has a public leaderboard ([s3prl/superb · Datasets at Hugging Face](https://huggingface.co/datasets/s3prl/superb#:~:text=Supported%20Tasks%20and%20Leaderboards)). As of recent, top models (often variants of Wav2Vec2 or Transformer-based encoders) achieve strong results like <5% PER on phonemes, ~95% accuracy on keyword spotting, and so on. The **SUPERB-SG** extension has added even more challenging tasks (speech translation, speech enhancement, etc.) ([SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark ...](https://sls.csail.mit.edu/publications/2022/JLai_ACL-22.pdf#:~:text=SUPERB,evaluating%20the%20semantic%20and%20gener)). Overall, SUPERB is akin to a “GLUE benchmark for speech,” testing robustness and versatility. It ensures that an LLM or audio model isn’t just good at transcribing speech, but can also understand who spoke, how they spoke, and what it means – crucial for comprehensive AI assistants that interact via voice.  

## Video Benchmarks (Video Understanding and Action Recognition)  

### Kinetics: Human Action Classification  
The **DeepMind Kinetics** dataset is a large-scale benchmark for **action recognition in video clips**. Kinetics comes in variants (Kinetics-400, -600, -700), where the number denotes how many human action classes are included. For example, Kinetics-400 has 400 distinct actions (like *playing guitar*, *swimming*, *kissing*, *riding a bike*) with about 300k video clips harvested from YouTube ([The Kinetics Human Action Video Dataset - arXiv.org](https://arxiv.org/pdf/1705.06950#:~:text=Statistics%3A%20The%20dataset%20has%20400,one%20for%20testing%20with)). Each clip is ~10 seconds long, focusing on the action in question. Models must output the correct action label for each clip. The evaluation metric is classification **top-1 and top-5 accuracy** (analogous to ImageNet). Kinetics was pivotal because it provided a *huge* and diverse video dataset – actions include human-object interactions and human-human interactions in varied contexts ([GitHub - cvdfoundation/kinetics-dataset](https://github.com/cvdfoundation/kinetics-dataset#:~:text=GitHub%20,Each%20action%20class)). A baseline “chance” accuracy is very low (0.25% for 400 classes). Early CNN-based video models (like 3D ConvNets and the I3D model) achieved around 70% top-1 on Kinetics-400. Over time, architectures evolved: 3D CNNs, then mixed Conv+Transformer models, and now pure Vision Transformers for video. With Kinetics as a benchmark, researchers report steady improvements. The current state-of-the-art on Kinetics-400 exceeds **85% top-1 accuracy** ([Videomae Huge Finetuned Kinetics · Models · Dataloop](https://dataloop.ai/library/model/mcg-nju_videomae-huge-finetuned-kinetics/#:~:text=Videomae%20Huge%20Finetuned%20Kinetics%20%C2%B7,model%20showcases%20the%20power)). For instance, a recent VideoMAE (Masked Autoencoder) model reached **86.6% top-1 (97.1% top-5)** on Kinetics-400 ([Videomae Huge Finetuned Kinetics · Models · Dataloop](https://dataloop.ai/library/model/mcg-nju_videomae-huge-finetuned-kinetics/#:~:text=Videomae%20Huge%20Finetuned%20Kinetics%20%C2%B7,model%20showcases%20the%20power)). These numbers indicate that for many actions (especially those that are visually distinctive, like *dunking a basketball*), models are extremely accurate. However, Kinetics includes some fine-grained classes that remain confusing, and there’s often a gap to human performance (humans might be ~95% on such a recognition task). The **methodology** is straightforward supervised learning – train on labeled 10s clips, predict the action. One limitation: Kinetics clips are usually well-centered on the action, and there’s one primary action label per clip. Thus, while it’s great for measuring a model’s general video recognition ability, it doesn’t test temporal reasoning beyond a few seconds, and it assumes one action is happening. It also doesn’t localize *where* or *when* the action occurs, only classifies the overall clip. Nonetheless, Kinetics benchmarks are key for video understanding, and models that excel on Kinetics often transfer well to other video tasks. It has effectively become the ImageNet of video, benchmarking the **general video feature learning** capability of models.

### AVA: Spatio-Temporal Action Localization  
The **AVA** dataset (Atomic Visual Actions) is a challenging benchmark that pushes video understanding a step further: not just *what* action is happening, but *who is doing what, when*. AVA consists of **15-minute movie clips** (mostly Hollywood films) in which actions are densely annotated at the person level ([GitHub - cvdfoundation/ava-dataset: The AVA dataset densely annotates 80 atomic visual actions in 351k movie clips with actions localized in space and time, resulting in 1.65M action labels with multiple labels per human occurring frequently.](https://github.com/cvdfoundation/ava-dataset#:~:text=AVA%20Actions%20Dataset)) ([GitHub - cvdfoundation/ava-dataset: The AVA dataset densely annotates 80 atomic visual actions in 351k movie clips with actions localized in space and time, resulting in 1.65M action labels with multiple labels per human occurring frequently.](https://github.com/cvdfoundation/ava-dataset#:~:text=The%20AVA%20dataset%20densely%20annotates,more%20details%20on%20the%20dataset)). Specifically, every person in key frames (one frame per second) is annotated with one or more action labels (from a set of 80 atomic actions) ([GitHub - cvdfoundation/ava-dataset: The AVA dataset densely annotates 80 atomic visual actions in 351k movie clips with actions localized in space and time, resulting in 1.65M action labels with multiple labels per human occurring frequently.](https://github.com/cvdfoundation/ava-dataset#:~:text=AVA%20Actions%20Dataset)) ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=Figure%201,in%20turn%20is%20determined%20by)). These actions are typically short, **fine-grained actions** like *standing*, *sitting*, *opening a door*, *throwing something*, etc., and multiple labels can apply (e.g. a person could be *standing* (pose) and *talking to another person* simultaneously) ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=Figure%201,in%20turn%20is%20determined%20by)). Moreover, each action annotation comes with a **spatial localization** (a bounding box around the person performing it) and implicitly a temporal localization (annotations cover each second of the clip) ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=research%20%28see%20Fig,%E2%88%97Google%20Research)) ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=Figure%201,in%20turn%20is%20determined%20by)). The evaluation metric for AVA is **mean Average Precision (mAP)**, similar to object detection, measured at an IoU threshold (e.g. IoU >0.5 between predicted and ground-truth person boxes) ([AVA: A Video Dataset of Atomic Visual Action - Google Search](https://research.google.com/ava/download.html#:~:text=Search%20research,trained%20Model)). A true positive requires correctly identifying the action and the person doing it in the frame. The task is exceedingly difficult: the model must detect people and classify their actions, often requiring context from the surrounding video frames to disambiguate actions ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=1%20pose%20action%20,seconds%20centered%20on%20a%20keyframe)). For example, distinguishing *picking up* vs *putting down* an object cannot be done from a single static image – you need to see the motion before/after ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=1%20pose%20action%20,seconds%20centered%20on%20a%20keyframe)). AVA provides a short snippet (3 seconds) around each key frame to help models incorporate temporal cues ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=frame%20containing%20multiple%20actors%20is,We%20keep%20the)). When AVA was introduced (CVPR 2018), baseline methods achieved only around **15–17% mAP** on the validation set ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=AVA%2C%20with%20its%20realistic%20scene,new%20ap%02proaches%20for%20video%20understanding)), highlighting its difficulty. State-of-the-art models have improved that significantly by using better person detectors and 3D ConvNets or Transformers for video: recent approaches (like Masked Video Distillation with ViT-Huge) have reached **~41% mAP** on AVA v2.2 ([[2212.04500] Masked Video Distillation: Rethinking Masked Feature ...](https://arxiv.org/abs/2212.04500#:~:text=For%20example%2C%20with%20the%20ViT,2)). Still, even ~40% is far from perfect (and far lower than detection mAP on COCO, for instance). Humans performing this task would likely get much higher mAP, though an exact human baseline is not widely reported; the complexity of some scenes (multiple people, subtle actions) makes it challenging even for humans. The **strength of AVA** as a benchmark is that it assesses a model’s **fine-grained understanding of human activities in context**: a single clip might have people *dancing* in the background while others are *talking* and *drinking* in the foreground, and a good model should localize each person and assign all applicable labels. This moves towards a more **comprehensive video understanding**, closer to how humans perceive and enumerate everything happening in a scene. Models that do well on AVA demonstrate strong spatio-temporal reasoning – the ability to not only recognize actions but also **when and where** they occur. AVA has influenced research in action localization and multi-label action recognition, complementing datasets like Kinetics.  

### “Something-Something”: Fine-Grained Human-Object Interactions  
The **Something-Something** dataset (V1 and V2) is a crowd-sourced video benchmark focusing on **common human-object interactions** ([The "Something Something" Video Database for Learning and Evaluating Visual Common Sense](https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf#:~:text=In%20this%20work%2C%20we%20introduce,Labels%20are)) ([The "Something Something" Video Database for Learning and Evaluating Visual Common Sense](https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf#:~:text=textual%20descriptions%20based%20on%20templates%2C,sets%20in)). Instead of generic labels like “cooking” or “playing guitar,” Something-Something has more descriptive, templated actions such as *“Moving something up,” “Moving something down,” “Putting something into something,” “Taking something out of something,”* etc. ([The "Something Something" Video Database for Learning and Evaluating Visual Common Sense](https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf#:~:text=textual%20descriptions%20based%20on%20templates%2C,sets%20in)). There are 174 action classes in V2, and each video (typically 2–6 seconds) involves a person performing that specific action with some object(s) ([The "Something Something" Video Database for Learning and Evaluating Visual Common Sense](https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf#:~:text=In%20this%20work%2C%20we%20introduce,Labels%20are)) ([The "Something Something" Video Database for Learning and Evaluating Visual Common Sense](https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf#:~:text=textual%20descriptions%20based%20on%20templates%2C,sets%20in)). The dataset contains over 220,000 videos in V2, collected by asking crowd workers to act out these predefined prompts with household objects ([The "Something Something" Video Database for Learning and Evaluating Visual Common Sense](https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf#:~:text=textual%20descriptions%20based%20on%20templates%2C,sets%20in)). The evaluation is **video classification**: given a video, predict which of the 174 actions is being shown. What makes Something-Something particularly challenging is the fine granularity and the requirement of **temporal modeling**. Many classes differ only by the temporal aspect or subtle details – e.g. *“pushing something to the left”* vs *“pushing something to the right”* involve the same objects and scene, only the direction differs. Likewise, *“covering something with something”* vs *“uncovering something”* are visually similar except for the temporal order. This means static image cues alone won’t solve it; the model must understand the sequence of frames. Metrics used are top-1 and top-5 accuracy. Early attempts with 3D CNNs yielded modest accuracy (around 30-40% top-1 for V1). As better architectures emerged (TSM, SlowFast, TimeSformer, etc.), results improved substantially. On Something-Something V2, recent transformer-based models with large-scale pretraining have achieved about **77% top-1 accuracy** ([[2212.04500] Masked Video Distillation: Rethinking Masked Feature ...](https://arxiv.org/abs/2212.04500#:~:text=For%20example%2C%20with%20the%20ViT,2)), which is the current state-of-the-art. This is on par with Kinetics performance despite the greater number of classes and the fine distinctions, highlighting the power of modern video models. However, the challenge is far from completely solved – a solid chunk of mistakes happen on interactions requiring common sense (e.g. differentiating *throwing* vs *spilling* something, which might look similar physically but have contextual differences). Humans viewing these videos (with the prompt in mind) can recognize the actions nearly perfectly; the benchmark is essentially testing if models can learn the “visual common sense” of object interactions ([The "Something Something" Video Database for Learning and Evaluating Visual Common Sense](https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf#:~:text=Something,Such)) ([The "Something Something" Video Database for Learning and Evaluating Visual Common Sense](https://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf#:~:text=In%20this%20work%2C%20we%20introduce,Labels%20are)). The **methodology** is standard supervised learning with a provided training set of labeled videos. One notable aspect is that Something-Something’s clips often center on hands manipulating objects, sometimes with only the hands visible. This bias forces models to really focus on motion and the objects’ state changes, rather than scene context. In summary, Something-Something is a **targeted benchmark for temporal reasoning and compositional actions** – it ensures that a high score means the model can discern subtle differences in *how* an action is done, not just static appearances. It complements datasets like Kinetics by covering the kind of physical interactions and **cause-effect understanding** that more generic datasets might miss.  

## Comparative Analysis of Benchmarks and Their Significance  

**Modality and Task Focus:** Each benchmark is tailored to specific AI tasks, and their applicability varies. For pure **language understanding**, GLUE and SuperGLUE provide a broad, task-agnostic check on a model’s grasp of syntax, semantics, and inference in text ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=single,pair%20NLU%20tasks%2C%20built)) ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=classification%20and%20similarity%20detection.%20,logical%20inference%20and%20contextual%20awareness)). They are highly relevant for general-purpose LLMs (e.g. a chatbot’s ability to understand and reason). MMLU, on the other hand, is focused on **factual knowledge and reasoning** across domains – excellent for evaluating a model’s education and breadth (important for models like GPT-4 which are used as knowledge experts) ([MMLU benchmark overview | arduin.io](https://arduin.io/blog/mmlu-overview/#:~:text=available%20benchmarks%20such%20as%20SuperGLUE,as%20listed%20in%20the%20paper)) ([What is GPT-4? | ITPro](https://www.itpro.com/technology/artificial-intelligence-ai/368288/what-is-gpt-4#:~:text=OpenAI%20also%20claims%20that%20GPT,3.5%E2%80%99s%2070%25%20and%20PaLM%E2%80%99s%2071)). In vision, ImageNet zeroes in on object recognition, which is crucial for any AI that needs basic vision (e.g. identifying what’s in an image). COCO and VQAv2 move up the complexity: COCO tests **multi-object detection and scene understanding**, making it more applicable to real-world vision systems like self-driving perception or robotics (where multiple objects must be tracked) ([COCO Dataset Resolution Insights and Applications](https://www.labellerr.com/blog/exploring-the-coco-dataset/#:~:text=The%20COCO%20dataset%20has%20become,instance%20segmentation%2C%20and%20image%20captioning)). VQAv2 explicitly blends vision and language, matching tasks for multimodal LLMs (like describing an image or answering questions about it) – a capability central to models like GPT-4 Vision. In audio, Librispeech addresses **ASR in clean conditions** – a foundational task for voice-based assistants, but somewhat narrow (read speech). Common Voice extends this to **robust, real-world ASR** across dialects and languages, so it’s more indicative of performance one might see in production voice applications (like voice search or dictation in various languages) ([Common Voice - Mozilla](https://commonvoice.mozilla.org/cv/datasets#:~:text=Common%20Voice%20,the%20Common%20Voice%20mailing%20list)) ([Common Voice - allizom.org](https://sandbox.commonvoice.allizom.org/en/datasets#:~:text=Common%20Voice%20,adding%20more%20voices%20and%20languages)). SUPERB ensures a model isn’t one-dimensional: a voice assistant, for example, must recognize *what* was said (ASR), *who* said it (speaker ID for personalization), and *how* (tone, which relates to emotion) – SUPERB’s multi-task evaluation directly checks such capabilities. In video, Kinetics is a general test of **action recognition**, useful for any AI interpreting human activities (surveillance systems, video tagging, etc.), whereas AVA and Something-Something step into **deeper reasoning**: AVA for multi-person, time-localized actions (e.g. understanding a complex scene with several people doing different things) and Something-Something for understanding **temporal order and causality** in physical interactions. These are relevant for advanced applications like video surveillance (which needs not just to detect people, but what each is doing) or assisting robots in understanding human instructions by demonstration.

**Robustness and Real-World Applicability:** Benchmarks also differ in how closely they reflect real-world complexity. On one end, GLUE’s tasks are relatively constrained (single sentences or sentence pairs with balanced datasets) – a model can score well by mastering those specific tasks, but that might not guarantee robustness to nuanced or open-ended language in the wild ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=Early%20GLUE%20models%20often%20found,rather%20than%20truly%20understanding%20language)). In contrast, SuperGLUE and MMLU are more robust indicators: SuperGLUE includes harder linguistic phenomena and even some **adversarially selected** data to prevent simple shortcuts ([GLUE Vs. SuperGLUE: How NLP Benchmarks Have Evolved](https://aicompetence.org/glue-vs-superglue-nlp-benchmarks/#:~:text=,examples%20to%20ensure%20robust%20evaluation)), and MMLU spans enough topics that a model can’t rely on narrow training data – it truly tests **generalization and retention of knowledge** ([MMLU: Better Benchmarking for LLM Language Understanding](https://deepgram.com/learn/mmlu-llm-benchmark-guide#:~:text=MMLU%3A%20Better%20Benchmarking%20for%20LLM,researchers%20to%20address%20weaknesses)). For vision, **ImageNet** (while pivotal) has been criticized for its narrow evaluation (single label, no context); models super-tuned to ImageNet sometimes falter on slightly shifted data (e.g. ImageNet-V2 or real-world images with multiple objects) ([Title: Re-assessing ImageNet: How aligned is its single-label ...](https://arxiv.org/abs/2412.18409#:~:text=This%20shift%20would%20enable%20a,on%20ImageNetV2)). **COCO and VQA**, by including multiple objects and free-form questions, push models closer to real-world vision-language understanding – a high score on COCO detection implies the model can handle diverse scenes and small objects, which correlates with better real-world performance (e.g. in a driving scene with cars and pedestrians). Common Voice is similarly “real-world” for speech: it intentionally incorporates speaker diversity and background noise, so it’s a robustness test – a model good on Common Voice is more likely to perform well for *actual users* with various accents, compared to one only trained/tested on LibriSpeech. And in video, **AVA’s multi-person, continuous annotations** mirror real security camera footage or movie understanding tasks, where multiple events occur and the AI must parse all concurrently ([GitHub - cvdfoundation/ava-dataset: The AVA dataset densely annotates 80 atomic visual actions in 351k movie clips with actions localized in space and time, resulting in 1.65M action labels with multiple labels per human occurring frequently.](https://github.com/cvdfoundation/ava-dataset#:~:text=AVA%20Actions%20Dataset)) ([AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf#:~:text=Figure%201,in%20turn%20is%20determined%20by)). Something-Something, while crowd-sourced and somewhat synthetic, touches on **physical common sense** (knowing that if I push something, it moves) – a capability crucial for embodied AI and reasoning about cause and effect.

**Complexity and Headroom:** Some benchmarks are largely solved or saturated, while others still have headroom. For example, **GLUE was effectively “defeated”** within a year (models >90, human ~87) ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=The%20human%20baseline%20score%20is,6)) ([Killed by LLM](https://r0bk.github.io/killedbyllm/#:~:text=Killed%205%20years%20ago%2C%20A,year%20and%201%20months%20old)), and even SuperGLUE now is nearing saturation (GPT-4 and beyond outperforming human baseline by several points) ([6 Cutting-Edge Pretrained Models for Text Classification in 2024](https://www.33rdsquare.com/6-pretrained-models-text-classification/#:~:text=On%20the%20SuperGLUE%20benchmark%2C%20a,a%20handful%20of%20labeled)). This means these benchmarks may no longer differentiate the top-tier models well – virtually all new large models max out the leaderboard. On the flip side, **MMLU and VQAv2 still have room**: GPT-4’s ~86% on MMLU is close to human, but not surpassing the estimated expert score ([Google's Gemini: Setting new benchmarks in language models](https://www.superannotate.com/blog/google-gemini-ai#:~:text=Pichai%20notes%20that%20on%20the,images%2C%20including%20reading%20comprehension)), and VQA’s ~77% is still below human ~85%, indicating models can still improve their reasoning to reach parity. **AVA’s 41% mAP** is far from any reasonable human performance (likely 80%+), so it remains a very hard problem – perfect for stress-testing the next generation of video models. **Something-Something’s ~77%** accuracy suggests models can understand many of the actions but not all (there’s 23% error to close, and those errors might be non-trivial ones requiring better temporal modeling or even multimodal knowledge). In terms of overall complexity ranking, one could say: *Text benchmarks* like GLUE are simpler (narrow range of tasks, short inputs), while *knowledge-heavy or reasoning benchmarks* like MMLU, Big-Bench are more complex and robust. *Image classification* (ImageNet) is simpler than *image understanding in context* (COCO, VQA). *Clean speech ASR* (LibriSpeech) is an easier slice than *diverse speech + understanding* (Common Voice, SUPERB which includes understanding intent/emotion). *Short video classification* (Kinetics) is more straightforward than *temporal and spatial localization* (AVA) or *fine-grained temporal reasoning* (Something-Something). Thus, depending on the AI system’s intended use, one would choose benchmarks appropriately. For an AI that just needs basic language skills, GLUE/SuperGLUE might suffice; but for an AI physician or lawyer, MMLU or domain-specific exam benchmarks are more relevant. For an image captioning bot, COCO Captions and VQA are important, whereas for an autonomous drone analyzing footage, something like Kinetics or AVA would be more applicable.

In practice, research and industry categorize benchmarks by these factors. Some create **“leaderboard tiers”**: e.g., *Tier 1:* Fundamentally solved benchmarks or those needed for basic competence (like GLUE, ImageNet, LibriSpeech) – any serious model is expected to have near state-of-the-art on these ([Killed by LLM](https://r0bk.github.io/killedbyllm/#:~:text=GLUE%282018%20)) ([Killed by LLM](https://r0bk.github.io/killedbyllm/#:~:text=Defeated%20by%3A%20XLNet)). *Tier 2:* Advanced benchmarks that distinguish the best from the rest (SuperGLUE, MMLU, COCO, VQA, Common Voice) – strong performance here indicates a robust and versatile model. *Tier 3:* Frontier benchmarks that even SOTA struggles with (e.g. BIG-bench, AVA, hard logical puzzles like ARC-AGI ([Benchmarks in AI: Measuring and Comparing Model Performance](https://www.forwardfuture.ai/p/benchmarks-in-artificial-intelligence-measuring-comparing-understanding#:~:text=%23%201%20ARC,Reasoning%20Corpus%20for%20AGI))) – performance here might be low across the board, so any improvement is notable and edges toward **AGI-like** reasoning ([Benchmarks in AI: Measuring and Comparing Model Performance](https://www.forwardfuture.ai/p/benchmarks-in-artificial-intelligence-measuring-comparing-understanding#:~:text=The%20rapid%20development%20of%20large,and%20what%20criticism%20they%20attract)). It’s also worth noting some benchmarks have **sub-benchmarks** within them (like SUPERB’s many subtasks, BIG-bench’s individual tasks), so strengths and weaknesses of models can be granularly analyzed.

Overall, no single benchmark is “best” – each illuminates different facets of an AI system. That’s why modern evaluations of models like GPT-4, PaLM-2, etc., report a **suite of benchmark results**, covering everything from basic language tasks to coding, math, vision, and speech. By looking at a collection of scores, we get a more complete picture of a model’s capabilities and limitations. Benchmarks thus act as **guideposts**: easy ones ensure baseline competency, and hard ones indicate how far we are from human-level or beyond-human performance in each domain.

## Tracking Latest Benchmark Results and Leaderboards  

Staying up-to-date with state-of-the-art model performance requires following a few key resources in real time. Many benchmarks have **public leaderboards** or dedicated websites. For instance, the GLUE and SuperGLUE leaderboards (at gluebenchmark.com and super.gluebenchmark.com) list top models and scores, though these benchmarks are now largely saturated ([Chapter 11 Resources and Benchmarks for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/resources-and-benchmarks-for-nlp.html#:~:text=For%20SuperGLUE%2C%20the%20human%20baseline,3)). Similarly, the COCO challenge website and VQA leaderboard (hosted on EvalAI) are updated when new models are submitted for competition each year. Beyond official sites, **Papers With Code** is an invaluable aggregator: it maintains leaderboards for hundreds of tasks including all the ones discussed (e.g. “ImageNet classification”, “VQAv2 test-std”, “LibriSpeech test-clean”) and highlights the current state-of-the-art model and paper ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=%2A%20MosaicML%20,HF%20Open%20LLM%20Leaderboard)). Researchers and practitioners often check PapersWithCode to see which model is on top and to get links to the relevant papers. Another resource is **Stanford’s HELM (Holistic Evaluation of Language Models)**, which is a living benchmark platform focusing on LLMs ([Holistic Evaluation of Language Models (HELM) - Stanford University](https://crfm.stanford.edu/helm/lite/latest/#:~:text=Holistic%20Evaluation%20of%20Language%20Models,website%20for%20exploration%20and%20study)). The HELM website compiles evaluation results of many models (OpenAI, Anthropic, Meta, etc.) across a standardized set of scenarios and metrics, and is updated as new models (like GPT-4, Claude, etc.) are evaluated ([Stanford CRFM](https://crfm.stanford.edu/2023/12/19/helm-lite.html#:~:text=Stanford%20CRFM%20HELM%20Lite%20is,the%20HELM%20framework%2C%20please)) ([Stanford CRFM](https://crfm.stanford.edu/2024/02/18/helm-instruct.html#:~:text=GPT,score%20in%2016%20of%20them)). HELM gives a broad comparison, not just on one number but on multiple axes (accuracy, calibration, bias, etc.), and it’s a great way to see how GPT-4.5 or **Gemini 2** would stack up once data is available.

For real-time news on the **latest benchmarks and models**: following venues like **arXiv (Artificial Intelligence category)** and **AI conference proceedings** is important, as new model papers will report their benchmark results. Often, when a model like “GPT-4.5” or “Grok 3” is released, the authors will release a report or paper including a table of benchmark scores. For example, OpenAI’s GPT-4 Technical Report included its scores on MMLU, SuperGLUE, HumanEval (coding), and more, which were widely cited ([What is GPT-4? | ITPro](https://www.itpro.com/technology/artificial-intelligence-ai/368288/what-is-gpt-4#:~:text=OpenAI%20also%20claims%20that%20GPT,3.5%E2%80%99s%2070%25%20and%20PaLM%E2%80%99s%2071)). Google’s announcements (e.g. at I/O or in blog posts) for **Gemini** or DeepMind’s models usually mention benchmark achievements – Sundar Pichai noted that Gemini surpassed GPT-4 on certain benchmarks like MMLU in text-only mode ([Google's Gemini: Setting new benchmarks in language models](https://www.superannotate.com/blog/google-gemini-ai#:~:text=Google%27s%20Gemini%3A%20Setting%20new%20benchmarks,images%2C%20including%20reading%20comprehension)). Industry research blogs such as the **OpenAI blog**, **Google AI blog**, **Meta AI blog**, and **Anthropic’s blog** often post benchmark comparisons when introducing new models (e.g. comparing to previous generation models or to humans). These are good sources for commentary and analysis beyond just numbers.

There are also community-driven and academic tracking efforts. The **EleutherAI LM Harness** and HuggingFace’s **Open LLM Leaderboard** continuously evaluate open-source models on benchmarks like MMLU, BoolQ, Hellaswag, etc., and allow the community to submit models for evaluation ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=%2A%20MosaicML%20,HF%20Open%20LLM%20Leaderboard)). The Open LLM Leaderboard (hosted on Hugging Face) ranks a variety of models by a aggregated score from many benchmarks and is updated as new open models (like Llama-2, etc.) come out ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=%2A%20MosaicML%20,HF%20Open%20LLM%20Leaderboard)). Additionally, sites like **Epoch AI** and **AI Model Leaderboards** compile data from papers and leaderboards to summarize progress. For example, one GitHub project “**LLM Leaderboard**” aggregates results from model papers, MosaicML’s charts, LMSys Chatbot Arena, and more into one place ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=The%20results%20of%20this%20leaderboard,is%20added%20as%20a%20link)) ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=%2A%20MosaicML%20,HF%20Open%20LLM%20Leaderboard)). Such compilations help compare models that might not all be on the same official leaderboard (since companies may not submit their closed models to public leaderboards, but we have their numbers from reports).

To follow analyses and commentary, one can read **newsletter posts and articles** by AI researchers who digest these results. For instance, the *Forward Future* blog (Feb 2025) discussed how GPT-4o, Claude 3.5, and Gemini 2.0 perform on benchmarks and what those benchmarks signify ([Benchmarks in AI: Measuring and Comparing Model Performance](https://www.forwardfuture.ai/p/benchmarks-in-artificial-intelligence-measuring-comparing-understanding#:~:text=The%20rapid%20development%20of%20large,and%20what%20criticism%20they%20attract)) ([Benchmarks in AI: Measuring and Comparing Model Performance](https://www.forwardfuture.ai/p/benchmarks-in-artificial-intelligence-measuring-comparing-understanding#:~:text=At%20the%20beginning%20of%20AI,sophisticated%20tests%20have%20become%20necessary)). Likewise, many in the AI community share “sota summaries” on Twitter (or X) and newsletters (e.g. *Import AI*, *BAI*, etc.), especially when a new record is set (like “Model X just hit a new high on VQA or beat human on SuperGLUE”). 

In summary, to track the latest: 

- **Official Leaderboard Sites**: (GLUE/SuperGLUE, COCO, VQA, Librispeech via ASR challenge sites, etc.) – give live competition results.  
- **Papers With Code**: One-stop for SOTA on specific datasets ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=%2A%20MosaicML%20,HF%20Open%20LLM%20Leaderboard)).  
- **Stanford HELM**: Ongoing evaluations of many LLMs side by side ([Holistic Evaluation of Language Models (HELM) - Stanford University](https://crfm.stanford.edu/helm/lite/latest/#:~:text=Holistic%20Evaluation%20of%20Language%20Models,website%20for%20exploration%20and%20study)).  
- **Hugging Face Leaderboards**: Especially for open models (Open LLM leaderboard, SpeechBrain for ASR, etc.).  
- **Research Papers/Tech Reports**: Check arXiv or company publications for GPT-4.5, Gemini, Grok, etc., where they will list benchmark results.  
- **Industry Blogs & Analysis**: OpenAI, DeepMind, Anthropic blogs for direct info; third-party analyses (Forward Future, etc.) for interpretation ([Benchmarks in AI: Measuring and Comparing Model Performance](https://www.forwardfuture.ai/p/benchmarks-in-artificial-intelligence-measuring-comparing-understanding#:~:text=The%20rapid%20development%20of%20large,and%20what%20criticism%20they%20attract)).  
- **Community Aggregators**: GitHub projects or forums that compile benchmark data from multiple sources ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=The%20results%20of%20this%20leaderboard,is%20added%20as%20a%20link)) ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=%2A%20MosaicML%20,HF%20Open%20LLM%20Leaderboard)).  

By consulting these sources, one can see in real time how the frontiers are moving – for instance, noticing that in late 2024 a certain model took the top spot in MMLU or that a new vision-language model set a record on VQAv2. The landscape of AI benchmarks is dynamic, so continuous monitoring is key. As new benchmarks emerge (for example, recently datasets for code generation, ethical reasoning, or multimodal reasoning are gaining importance), these sources will also begin tracking those, giving a comprehensive view of progress in AI capabilities. Each benchmark conquered is a story of technological progress – and the community collectively watches these leaderboards to gauge how fast we’re approaching truly general AI.  ([GitHub - LudwigStumpp/llm-leaderboard: A joint community effort to create one central leaderboard for LLMs.](https://github.com/LudwigStumpp/llm-leaderboard#:~:text=Special%20thanks%20to%20the%20following,pages)) ([Benchmarks in AI: Measuring and Comparing Model Performance](https://www.forwardfuture.ai/p/benchmarks-in-artificial-intelligence-measuring-comparing-understanding#:~:text=The%20rapid%20development%20of%20large,and%20what%20criticism%20they%20attract))

