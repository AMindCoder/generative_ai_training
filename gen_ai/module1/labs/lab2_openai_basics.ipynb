{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Getting Started with OpenAI API\n",
    "\n",
    "## Objectives\n",
    "- Learn to use OpenAI's API for text generation\n",
    "- Understand different model parameters\n",
    "- Create basic applications using the API\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 1\n",
    "- OpenAI API key configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Text Generation\n",
    "Let's start with simple text generation tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test with different prompts\n",
    "prompts = [\n",
    "    \"Write a haiku about artificial intelligence\",\n",
    "    \"Explain quantum computing to a 5-year-old\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(f\"Response:\\n{generate_text(prompt)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Temperature and Top-p\n",
    "Experiment with different generation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_temperatures(prompt, temperatures=[0.2, 0.7, 1.0]):\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    for temp in temperatures:\n",
    "        print(f\"Temperature: {temp}\")\n",
    "        print(f\"Response: {generate_text(prompt, temperature=temp)}\\n\")\n",
    "\n",
    "compare_temperatures(\"Write a creative story about a robot learning to paint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Simple Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_system(context, question):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the given context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test the Q&A system\n",
    "context = \"\"\"\n",
    "Generative AI models are artificial intelligence systems that can create new content,\n",
    "including text, images, music, and code. These models learn patterns from training\n",
    "data and use that knowledge to generate new, original content that follows similar patterns.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What can generative AI models create?\",\n",
    "    \"How do these models generate new content?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {qa_system(context, question)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Code Generation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful coding assistant. Provide code solutions in Python.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "code_prompt = \"Write a Python function that calculates the Fibonacci sequence up to n terms\"\n",
    "print(generate_code(code_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create a chatbot that maintains conversation context using the chat completions API\n",
    "2. Experiment with different temperature values and observe the variations in output\n",
    "3. Build a simple code review system using the OpenAI API\n",
    "4. Create a text summarization function for long articles\n",
    "\n",
    "## Next Steps\n",
    "- Explore more advanced features of the OpenAI API\n",
    "- Learn about token usage and optimization\n",
    "- Try different models and compare their performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
