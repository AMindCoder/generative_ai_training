{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Chaining Prompts with LangChain\n",
    "\n",
    "## Objectives\n",
    "- Learn LangChain fundamentals\n",
    "- Build multi-step prompt chains\n",
    "- Implement memory in conversations\n",
    "- Create custom tools and agents\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Labs 1 and 2\n",
    "- Understanding of prompt engineering concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Prompt Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for generating a story idea\n",
    "story_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a short story idea about {topic}.\"\n",
    ")\n",
    "\n",
    "# Create a chain for generating a title\n",
    "title_prompt = PromptTemplate(\n",
    "    input_variables=[\"story\"],\n",
    "    template=\"Generate a creative title for this story: {story}\"\n",
    ")\n",
    "\n",
    "# Create individual chains\n",
    "story_chain = LLMChain(llm=llm, prompt=story_prompt)\n",
    "title_chain = LLMChain(llm=llm, prompt=title_prompt)\n",
    "\n",
    "# Combine chains\n",
    "story_generation_chain = SimpleSequentialChain(\n",
    "    chains=[story_chain, title_chain],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "result = story_generation_chain.run(\"artificial intelligence\")\n",
    "print(\"Final Title:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalAssistant:\n",
    "    def __init__(self):\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \"human_input\"],\n",
    "            template=\"\"\"Previous conversation:\n",
    "            {chat_history}\n",
    "            \n",
    "            Human: {human_input}\n",
    "            Assistant: \"\"\"\n",
    "        )\n",
    "        \n",
    "        self.conversation = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=self.prompt,\n",
    "            memory=self.memory,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        return self.conversation.predict(human_input=user_input)\n",
    "\n",
    "# Test the conversational assistant\n",
    "assistant = ConversationalAssistant()\n",
    "\n",
    "# Have a multi-turn conversation\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you give me an example?\",\n",
    "    \"How is that different from deep learning?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nHuman: {question}\")\n",
    "    response = assistant.chat(question)\n",
    "    print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Custom Tools and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from typing import List\n",
    "\n",
    "class CodeAssistant:\n",
    "    def __init__(self):\n",
    "        # Define custom tools\n",
    "        self.tools = [\n",
    "            Tool(\n",
    "                name=\"code_generator\",\n",
    "                func=self._generate_code,\n",
    "                description=\"Generates Python code based on a description\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"code_explainer\",\n",
    "                func=self._explain_code,\n",
    "                description=\"Explains what a piece of code does\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"code_reviewer\",\n",
    "                func=self._review_code,\n",
    "                description=\"Reviews code and suggests improvements\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Initialize the agent\n",
    "        self.agent = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    def _generate_code(self, description: str) -> str:\n",
    "        prompt = f\"Generate Python code that does the following: {description}\"\n",
    "        return llm(prompt)\n",
    "    \n",
    "    def _explain_code(self, code: str) -> str:\n",
    "        prompt = f\"Explain what this code does in simple terms:\\n{code}\"\n",
    "        return llm(prompt)\n",
    "    \n",
    "    def _review_code(self, code: str) -> str:\n",
    "        prompt = f\"Review this code and suggest improvements:\\n{code}\"\n",
    "        return llm(prompt)\n",
    "    \n",
    "    def assist(self, query: str) -> str:\n",
    "        return self.agent.run(query)\n",
    "\n",
    "# Test the code assistant\n",
    "assistant = CodeAssistant()\n",
    "\n",
    "queries = [\n",
    "    \"Generate a Python function to calculate the fibonacci sequence\",\n",
    "    \"Explain what this code does: def quicksort(arr): return arr if len(arr) <= 1 else quicksort([x for x in arr[1:] if x <= arr[0]]) + [arr[0]] + quicksort([x for x in arr[1:] if x > arr[0]])\",\n",
    "    \"Review this code: def process(x): return x + 1\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    response = assistant.assist(query)\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Research Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "class ResearchAssistant:\n",
    "    def __init__(self):\n",
    "        # Initialize search tool\n",
    "        self.search = DuckDuckGoSearchRun()\n",
    "        \n",
    "        # Create tools list\n",
    "        self.tools = [\n",
    "            Tool(\n",
    "                name=\"web_search\",\n",
    "                func=self.search.run,\n",
    "                description=\"Useful for searching the web for current information\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"summarize\",\n",
    "                func=self._summarize_text,\n",
    "                description=\"Summarizes a piece of text\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Initialize the agent\n",
    "        self.agent = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    def _summarize_text(self, text: str) -> str:\n",
    "        prompt = f\"Summarize the following text:\\n{text}\"\n",
    "        return llm(prompt)\n",
    "    \n",
    "    def research(self, query: str) -> str:\n",
    "        return self.agent.run(query)\n",
    "\n",
    "# Test the research assistant\n",
    "researcher = ResearchAssistant()\n",
    "\n",
    "research_queries = [\n",
    "    \"What are the latest developments in quantum computing?\",\n",
    "    \"Summarize the key benefits of using LangChain for AI development\"\n",
    "]\n",
    "\n",
    "for query in research_queries:\n",
    "    print(f\"\\nResearch Query: {query}\")\n",
    "    response = researcher.research(query)\n",
    "    print(f\"Findings: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create a multi-step chain for processing and analyzing text data\n",
    "2. Build an agent that can interact with multiple APIs\n",
    "3. Implement a document question-answering system using LangChain\n",
    "4. Create a custom tool for specific domain tasks\n",
    "\n",
    "## Next Steps\n",
    "- Explore more advanced LangChain features\n",
    "- Implement custom memory systems\n",
    "- Create domain-specific agents\n",
    "- Build more complex tool combinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
